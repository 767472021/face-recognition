#!/usr/bin/env python3

######################
# image_to_vector.py #
######################


# 3 out of 6 functions are used in main.py
# In short, they convert input images to 128-d vectors and save them
#
# Details:
# 1. gen_vector_initialize
#      - Generate all vectors from all images
#      - For initialization / re-initialization
#      - WARNING: All pickle files would be deleted!!!
#
# 2. gen_vector_register
#      - Generate all vectors from images of a single member
#      - For registration
#
# 3. gen_vector_add
#      - Generate vectors from images that has not been
#        converted to vector of a single member
#      - For adding new photos 



import numpy as np
import imutils
import pickle
import cv2
import os
import shutil

import load_cnn
import pre_recognition


def gen_vector_initialize(min_prob_filter):
    '''
    For initialization and re-initialization.
    All 128-d vectors and SVM pickle files will be deleted
    
    0. Loops all images in the dataset,
    1. Convert them to 128-d vectors,
    2. Save the serialized dictionary (image_paths, 128-d vectors)
         - for individual identity (path:  embeddings/data/individual/<identity>/embeddings.pickle)
         - for all                 (path:  embeddings/data/overall/embeddings.pickle)

    Arguments:
    1. min_prob_filter:     Probability threshold to filter weak detection by ResNet

    Returns:
    1. num_identity:        Number of unique identity in dataset
    '''

    print("#################################")
    print("#################################")
    print("#####    Initialization     #####") 
    print("#####    Reset all data     #####") 
    print("#################################")
    print("#################################\n")

    DATASET_PATH = "dataset"
    EMBEDDING_OUTPUT = "embeddings/data"
    FILE_NAME = "embeddings.pickle"
    INDIVIDUAL = "individual"
    OVERALL = "overall"

    SVM_PATH = "face_recognition_model/svm"
    KNN_PATH = "face_recognition_model/knn"
    RF_PATH = "face_recognition_model/rf"

    # NOTE: This part is unique in this function
    #       It clears up all the pickle files
    #
    # Delete all pickle files that store 128-d vectors of each identity
    individual_path = os.path.join(EMBEDDING_OUTPUT, INDIVIDUAL)
    if os.path.isdir(individual_path):
        print("Delete all 128-d vectors for each identity")
        shutil.rmtree(individual_path)
    os.makedirs(individual_path)

    # Delete the pickle file that stores 128-d vectors of all identites
    # in embeddings/data/overall/embeddings.pickle
    overall_path = os.path.join(EMBEDDING_OUTPUT, OVERALL, FILE_NAME)
    if os.path.isfile(overall_path):
        print("Delete 128-d vectors in overall directory")
        os.remove(overall_path)
   
    # Delete all pickle files generated by SVM
    if os.path.isdir(SVM_PATH):
        print("Delete Support Vector Machine Model")
        shutil.rmtree(SVM_PATH)
    os.makedirs(SVM_PATH)


    # Delete all pickle files generated by KNN
    if os.path.isdir(KNN_PATH):
        print("Delete k-Nearest Neighbours Model")
        shutil.rmtree(KNN_PATH)
    os.makedirs(KNN_PATH)


    # Delete all pickle files generated by Random Forest
    if os.path.isdir(RF_PATH):
        print("Delete Random Forest Model")
        shutil.rmtree(RF_PATH)
    os.makedirs(RF_PATH)




    # Load the CNN for face detection and 128-d vector extraction
    detector = load_cnn.resnet()
    embedder = load_cnn.facenet()

    # Initialize list to store image path and 128-d vector
    # for ALL identities
    overall_image_paths = []
    overall_vectors = []

    # Total number of vectors for all identities
    vector_num_all = 0

    # Total number of image processed
    image_num_all = 0
    
    # Get all unique identities
    identities = [x for x in os.listdir(DATASET_PATH) if not x.startswith('.')]
    identities = sorted(identities)
    num_identity = len(identities)

    # Loop over all identities
    for identity in identities:
        print("\n*** Processing images with identity: " + identity + " ***")
        
        # Create directory for THIS identity to store 128-d vector
        vector_path = os.path.join(individual_path, identity)
        if not os.path.isdir(vector_path):
            os.makedirs(vector_path)
        else:
            print("WARNING: The path " + vector_path + " exists")
            print("         Removing all its contents")
            shutil.rmtree(vector_path)
            os.makedirs(vector_path)


        # Initialize list to store image path and 128-d vector
        # for THIS identity
        individual_image_paths = []
        individual_vectors = []

        # Total number of vector for THIS identity
        vector_num = 0

        # Setting up image path
        identity_path = os.path.join(DATASET_PATH, identity)
        image_files = [x for x in os.listdir(identity_path) if not x.startswith('.')]
        image_num = len(image_files)
        image_num_all += image_num


        # Loop over images for THIS identity
        for image_file in image_files: 
            image_path = os.path.join(identity_path, image_file)

            # Extract 128-d vector from the image
            vector = core(image_path, detector, embedder, min_prob_filter)

            # Skip below if vector is not extracted
            if np.isfinite(vector).all() == False:
                print("WARNING: No 128-d vector extracted for " + image_path)
                continue                

            # For individual member
            individual_image_paths.append(image_path)
            individual_vectors.append(vector)

            # For overall
            overall_image_paths.append(image_path)
            overall_vectors.append(vector)

            vector_num += 1
            vector_num_all +=1

        # Save the extracted 128-d vectors for THIS identity
        print("Serializing {} 128-d vectors for {} images".format(vector_num, image_num))
        output_path = os.path.join(EMBEDDING_OUTPUT, INDIVIDUAL, identity, FILE_NAME)
        data = dict(zip(individual_image_paths, individual_vectors))
        serialize(data, output_path)

        print("*** Done with identity: " + identity + " ***\n")

    # Save the extracted 128-d vectors for ALL identities
    print("****** Combining All vectors ******")
    print("Serializing {} 128-d vectors for {} images".format(vector_num_all, image_num_all))
    output_path = os.path.join(EMBEDDING_OUTPUT, OVERALL, FILE_NAME)
    data = dict(zip(overall_image_paths, overall_vectors))
    serialize(data, output_path)
    print("****** Initialization completed!!! ******")

    return num_identity





def gen_vector_register(identity, min_prob_filter):
    '''
    For registration
    
    0. Loops all images in the dataset/<identity>,
    1. Convert them to 128-d vectors,
    2. Save the serialized dictionary (image_paths, 128-d vectors)
       to path:  embeddings/data/individual/<identity>/embeddings.pickle)

    Arguments:
    1. identity:            Name for registration

    2. min_prob_filter:     Probability threshold to filter weak detection by ResNet

    '''
    print("##########################")
    print("##########################")
    print("#####   New vector   #####")
    print("#####   New member   #####")
    print("##########################")
    print("##########################\n")

    DATASET_PATH = "dataset"
    EMBEDDING_OUTPUT = "embeddings/data"
    FILE_NAME = "embeddings.pickle"
    INDIVIDUAL = "individual"
    OVERALL = "overall"


    # Load the CNN for face detection and 128-d vector extraction
    detector = load_cnn.resnet()
    embedder = load_cnn.facenet()

    print("\n*** Processing images with identity: " + identity + " ***")
        
    # Initialize list to store image path and 128-d vector
    # for THIS identity
    individual_image_paths = []
    individual_vectors = []

    # Total number of vector for THIS identity
    vector_num = 0

    # Setting up image path
    identity_path = os.path.join(DATASET_PATH, identity)
    image_files = [x for x in os.listdir(identity_path) if not x.startswith('.')]
    image_num = len(image_files)

    # Create directory for THIS identity to store 128-d vector
    vector_path = os.path.join(EMBEDDING_OUTPUT, INDIVIDUAL, identity)
    if not os.path.isdir(vector_path):
        os.makedirs(vector_path)
    else:
        print("WARNING: The path " + vector_path + " exists")
        print("         Removing all its contents")
        shutil.rmtree(vector_path)
        os.makedirs(vector_path)
    


    # Loop over images for THIS identity
    for image_file in image_files: 
        image_path = os.path.join(identity_path, image_file)

        # Extract 128-d vector from the image
        vector = core(image_path, detector, embedder, min_prob_filter)

        # Skip below if vector is not extracted
        if np.isfinite(vector).all() == False:
            print("WARNING: No 128-d vector extracted for " + image_path)
            continue                

        # For individual member
        individual_image_paths.append(image_path)
        individual_vectors.append(vector)

        vector_num += 1

    # Save the extracted 128-d vectors for THIS identity
    print("Serializing {} 128-d vectors for {} images".format(vector_num, image_num))
    output_path = os.path.join(EMBEDDING_OUTPUT, INDIVIDUAL, identity, FILE_NAME)
    data_new = dict(zip(individual_image_paths, individual_vectors))
    serialize(data_new, output_path)

    # NOTE: Do not forget to append the data for the overall file
    output_path = os.path.join(EMBEDDING_OUTPUT, OVERALL, FILE_NAME)
    data_all = deserialize(output_path)
    data_all.update(data_new)
    serialize(data_all, output_path)
    print("*** Done with identity: " + identity + " ***\n")



def gen_vector_add(identity, min_prob_filter):
    '''
    For adding new photos
    
    0. Loops images in the dataset/<identity> that are not yet converted to 128-d vectors 
    1. Convert them to 128-d vectors
    2. Append and then save the serialized dictionary (image_paths, 128-d vectors)
       to path:  embeddings/data/individual/<identity>/embeddings.pickle)

    Arguments:
    1. identity:            Name for registration

    2. min_prob_filter:     Probability threshold to filter weak detection by ResNet

    '''
    print("#############################")
    print("#############################")
    print("#####     New vector    #####")
    print("#####   Current member  #####")
    print("#############################")
    print("#############################\n")

    DATASET_PATH = "dataset"
    EMBEDDING_OUTPUT = "embeddings/data"
    FILE_NAME = "embeddings.pickle"
    INDIVIDUAL = "individual"
    OVERALL = "overall"


   
    # Load the CNN for face detection and 128-d vector extraction
    detector = load_cnn.resnet()
    embedder = load_cnn.facenet()

    print("\n*** Processing images with identity: " + identity + " ***")
    
    # Initialize list to store image path and 128-d vector
    # for THIS identity
    individual_image_path = []
    individual_vector = []

    # Total number of vector for THIS identity
    vector_num = 0

    # Setting up image path
    identity_path = os.path.join(DATASET_PATH, identity)
    image_files = [x for x in os.listdir(identity_path) if not x.startswith('.')]


    # NOTE: Extra part to handle new photos
    # Load the embeddings.pickle of THIS identity
    output_path = os.path.join(EMBEDDING_OUTPUT, INDIVIDUAL, identity, FILE_NAME)
    data = deserialize(output_path)
    
    # Get the image names that have been converted to 128-d vectors
    image_processed = [f.split(os.sep)[-1] for f in list(data.keys())]

    # Replace image_files (all) to a subset that has not yet been
    # converted to 128-d vectors
    image_files = set(image_files).difference(set(image_processed))
    image_num = len(image_files)


    # Loop over images for this identity
    for image_file in image_files: 
        image_path = os.path.join(identity_path, image_file)

        # Extract 128-d vector from the image
        vector = core(image_path, detector, embedder, min_prob_filter)

        # Skip below if vector is not extracted
        if np.isfinite(vector).all() == False:
            print("WARNING: No 128-d vector extracted for " + image_path)
            continue                

        # For individual member
        individual_image_path.append(image_path)
        individual_vector.append(vector)

        vector_num += 1

    # Save the extracted 128-d vectors for THIS identity
    print("Serializing {} 128-d vectors for {} images".format(vector_num, image_num))
    
    # NOTE: Extra part to handle new vectors from new photos
    # Append new_data (new 128-d vector) to data (existing 128-d vector)
    data_new = dict(zip(individual_image_path, individual_vector))
    data.update(data_new)
    serialize(data, output_path)

    # NOTE: Do not forget to append the data for the overall file
    output_path = os.path.join(EMBEDDING_OUTPUT, OVERALL, FILE_NAME)
    data_all = deserialize(output_path)
    data_all.update(data_new)
    serialize(data_all, output_path)
    
    print("*** Done with identity: " + identity + " ***\n")





def deserialize(path):
    '''
    Deserialize the pickle file

    Arguments:
    1. path:  Path that stored the pickle file

    Returns:
    1. data:        Deserialized object
    '''
    with open(path, "rb") as f:
        data = pickle.load(f)

    return data




def serialize(data, path):
    '''
    Serialize the data and save to disk

    Arguments:
    1. data:            Data to serialize and save

    2. output_path:     Path to save the file
    '''
    with open(path, "wb") as f:
        pickle.dump(data, f)
 




def core(image_path, detector, embedder, min_prob_filter):
    '''
    Convert an image to a 128-d vector

    Arguments:
    1. image_path:          Image path

    2. detector:            ResNet CNN

    3. embedder:            FaceNet CNN

    4. min_prob_filter:     Probability threshold to filter weak detection
                            from ResNet CNN

    Returns:
    1. vector:              128-d vector (can be None if no detection)

    '''

    # Initialize the 128-d vector
    vector = np.nan

    # Load the image and resize to width of 600 pixels,
    # while maintaing the aspect ratio. Then, get the
    # image dimension
    image = cv2.imread(image_path)
    image = imutils.resize(image, width=600)
    (h, w) = image.shape[:2]

    # Generate bounding boxes for face candidates
    detections, _ = pre_recognition.locate_faces(image, detector, None)

    # Ensure at least one face candidate
    if len(detections) > 0:
        # NOTE: Assume each image has only ONE face !!!!
        #       Select the bounding box with the largest probability
        #       Here, only ONE bounding box is chosen !
        i = np.argmax(detections[0, 0, :, 2])
        confidence = detections[0, 0, i, 2]

        # Filter weak detection
        if confidence > min_prob_filter:
            # Compute the (x, y) coordinates of the bounding box
            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
            (start_x, start_y, end_x, end_y) = box.astype("int")

            # Crop the ROI and dimensions
            face = image[start_y:end_y, start_x:end_x]
            (face_h, face_w) = face.shape[:2]

            # Filter small face candidates
            if face_w >= 20 and face_h >= 20:
                # Extract the 128-d vector from the ROI
                vector, _ = pre_recognition.extract_vector(face, embedder, None)

    return vector




